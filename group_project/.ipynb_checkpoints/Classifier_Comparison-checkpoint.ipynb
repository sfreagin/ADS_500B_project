{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695e8e34",
   "metadata": {},
   "source": [
    "## Initial Prediction Model\n",
    "The original README file says:\n",
    "> Often, more than one contact to the same client was required, **in order to access if the product (bank term deposit) would be (or not) subscribed**\n",
    "\n",
    "Therefore, let's start with a simple binary classification model to predict Deposit yes/no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7bf69acd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:37.658260Z",
     "iopub.status.busy": "2022-08-02T02:48:37.657263Z",
     "iopub.status.idle": "2022-08-02T02:48:37.758294Z",
     "shell.execute_reply": "2022-08-02T02:48:37.758294Z",
     "shell.execute_reply.started": "2022-08-02T02:48:37.658260Z"
    }
   },
   "outputs": [],
   "source": [
    "#import the right libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, log_loss, roc_auc_score, hamming_loss, fbeta_score, auc, roc_curve, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, learning_curve, GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, LabelEncoder, RobustScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0837eb3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:37.808258Z",
     "iopub.status.busy": "2022-08-02T02:48:37.808258Z",
     "iopub.status.idle": "2022-08-02T02:48:37.907299Z",
     "shell.execute_reply": "2022-08-02T02:48:37.907299Z",
     "shell.execute_reply.started": "2022-08-02T02:48:37.808258Z"
    }
   },
   "outputs": [],
   "source": [
    "#this option just allwos us to see every column in the notebook\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#pd.get_option(\"display.max_columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "098561a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:37.927260Z",
     "iopub.status.busy": "2022-08-02T02:48:37.927260Z",
     "iopub.status.idle": "2022-08-02T02:48:38.102291Z",
     "shell.execute_reply": "2022-08-02T02:48:38.102291Z",
     "shell.execute_reply.started": "2022-08-02T02:48:37.927260Z"
    }
   },
   "outputs": [],
   "source": [
    "#pull in the dataset and turn into a DataFrame\n",
    "bank_main_df = pd.read_csv('./Dataset_1_Bank Marketing/bank_marketing.csv',delimiter=';')\n",
    "# bank_main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "468c0585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:38.104258Z",
     "iopub.status.busy": "2022-08-02T02:48:38.103258Z",
     "iopub.status.idle": "2022-08-02T02:48:38.202257Z",
     "shell.execute_reply": "2022-08-02T02:48:38.201257Z",
     "shell.execute_reply.started": "2022-08-02T02:48:38.104258Z"
    }
   },
   "outputs": [],
   "source": [
    "# bank_main_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ef39a1f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:38.203257Z",
     "iopub.status.busy": "2022-08-02T02:48:38.203257Z",
     "iopub.status.idle": "2022-08-02T02:48:38.309257Z",
     "shell.execute_reply": "2022-08-02T02:48:38.309257Z",
     "shell.execute_reply.started": "2022-08-02T02:48:38.203257Z"
    }
   },
   "outputs": [],
   "source": [
    "#checking the options available under the \"deposit\" field\n",
    "# bank_main_df['deposit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "32d3524b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:38.458261Z",
     "iopub.status.busy": "2022-08-02T02:48:38.457259Z",
     "iopub.status.idle": "2022-08-02T02:48:38.742261Z",
     "shell.execute_reply": "2022-08-02T02:48:38.741259Z",
     "shell.execute_reply.started": "2022-08-02T02:48:38.458261Z"
    }
   },
   "outputs": [],
   "source": [
    "#replacing the yes/no categorical values with 1/0 binary digits\n",
    "bank_main_df['deposit'] = [1 if (bank_main_df['deposit'][i] == 'yes') else 0 for i in range(len(bank_main_df)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "200492f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:38.743259Z",
     "iopub.status.busy": "2022-08-02T02:48:38.742261Z",
     "iopub.status.idle": "2022-08-02T02:48:38.874308Z",
     "shell.execute_reply": "2022-08-02T02:48:38.874308Z",
     "shell.execute_reply.started": "2022-08-02T02:48:38.743259Z"
    }
   },
   "outputs": [],
   "source": [
    "#because we have so many cateogrical variables, we should one-hot encode them (i.e. create dummy categorical variables)\n",
    "#we also use drop_first=True to reduce the redundant column count \n",
    "bank_main_df = pd.get_dummies(bank_main_df, drop_first=False)\n",
    "\n",
    "# bank_main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ef8251dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:39.076264Z",
     "iopub.status.busy": "2022-08-02T02:48:39.075263Z",
     "iopub.status.idle": "2022-08-02T02:48:39.180215Z",
     "shell.execute_reply": "2022-08-02T02:48:39.179213Z",
     "shell.execute_reply.started": "2022-08-02T02:48:39.076264Z"
    }
   },
   "outputs": [],
   "source": [
    "#note that only the \"age\" category has null values\n",
    "\n",
    "# pd.isnull(bank_main_df).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50f2a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:22:26.864109Z",
     "iopub.status.busy": "2022-08-02T02:22:26.864109Z",
     "iopub.status.idle": "2022-08-02T02:22:26.995149Z",
     "shell.execute_reply": "2022-08-02T02:22:26.995149Z",
     "shell.execute_reply.started": "2022-08-02T02:22:26.864109Z"
    }
   },
   "source": [
    "# Imputing the missing values in \"Age\" variable \n",
    "\n",
    "* **Iterative Imputer:**\n",
    "Multivariate imputer that estimates each feature from all the others. A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ea6ce845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:49:07.295237Z",
     "iopub.status.busy": "2022-08-02T02:49:07.295237Z",
     "iopub.status.idle": "2022-08-02T02:49:18.504566Z",
     "shell.execute_reply": "2022-08-02T02:49:18.504566Z",
     "shell.execute_reply.started": "2022-08-02T02:49:07.295237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use multivariate imputer that estimates and imputes null values based on all the others. \n",
    "\n",
    "imp = IterativeImputer(max_iter=10, verbose=0) # values passed are defaults, but added them because they seem important... play around\n",
    "imp.fit(bank_main_df)\n",
    "imputed_df = imp.transform(bank_main_df)\n",
    "imputed_df = pd.DataFrame(imputed_df, columns=bank_main_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "eb889199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:49:37.415797Z",
     "iopub.status.busy": "2022-08-02T02:49:37.414783Z",
     "iopub.status.idle": "2022-08-02T02:49:37.571779Z",
     "shell.execute_reply": "2022-08-02T02:49:37.570779Z",
     "shell.execute_reply.started": "2022-08-02T02:49:37.415797Z"
    }
   },
   "outputs": [],
   "source": [
    "# pd.isnull(imputed_df).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb1b28",
   "metadata": {},
   "source": [
    "# R-Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fb10afd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:41.444462Z",
     "iopub.status.busy": "2022-08-02T02:48:41.444462Z",
     "iopub.status.idle": "2022-08-02T02:48:41.618503Z",
     "shell.execute_reply": "2022-08-02T02:48:41.618503Z",
     "shell.execute_reply.started": "2022-08-02T02:48:41.444462Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "X_scaled = scaler.fit_transform(imputed_df)\n",
    "\n",
    "X = imputed_df.drop(columns='deposit')\n",
    "y = imputed_df['deposit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, shuffle=True, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d906b58",
   "metadata": {},
   "source": [
    "# Loop through all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "94aa52ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:42.073676Z",
     "iopub.status.busy": "2022-08-02T02:48:42.073676Z",
     "iopub.status.idle": "2022-08-02T02:48:42.243926Z",
     "shell.execute_reply": "2022-08-02T02:48:42.243070Z",
     "shell.execute_reply.started": "2022-08-02T02:48:42.073676Z"
    }
   },
   "outputs": [],
   "source": [
    "# StandardScaler, MinMaxScaler, RobustScaler\n",
    "scaler = RobustScaler() \n",
    "X_scaled = scaler.fit_transform(imputed_df)\n",
    "\n",
    "X = imputed_df.drop(columns='deposit')\n",
    "y = imputed_df['deposit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c21430af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:42.423846Z",
     "iopub.status.busy": "2022-08-02T02:48:42.423846Z",
     "iopub.status.idle": "2022-08-02T02:48:42.561845Z",
     "shell.execute_reply": "2022-08-02T02:48:42.560880Z",
     "shell.execute_reply.started": "2022-08-02T02:48:42.423846Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-bc12ea19ccdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m#     y_pred = clf.predict(X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WPy64-3890\\python-3.8.9.amd64\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnearest\u001b[0m \u001b[0mneighbors\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WPy64-3890\\python-3.8.9.amd64\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"requires_y\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m                 X, y = self._validate_data(X, y, accept_sparse=\"csr\",\n\u001b[0m\u001b[0;32m    364\u001b[0m                                            multi_output=True)\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WPy64-3890\\python-3.8.9.amd64\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WPy64-3890\\python-3.8.9.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WPy64-3890\\python-3.8.9.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WPy64-3890\\python-3.8.9.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WPy64-3890\\python-3.8.9.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WPy64-3890\\python-3.8.9.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "\n",
    "classifiers = [\n",
    "#     MultinomialNB(), # doesn't work\n",
    "    KNeighborsClassifier(3), # works\n",
    "#     SVC(kernel=\"rbf\", C=0.001, probability=True), # took a long time... need to refresh memory\n",
    "#     SVC(kernel='linear'), # took a long time... need to refresh memory\n",
    "#     NuSVC(probability=True, nu=0.1), # took a long time... need to refresh memory\n",
    "    DecisionTreeClassifier(), # works\n",
    "    RandomForestClassifier(), # works\n",
    "    AdaBoostClassifier(), # works\n",
    "    GradientBoostingClassifier(), # works\n",
    "    GaussianNB(), # works\n",
    "    BernoulliNB(), # works\n",
    "    MLPClassifier(), # works\n",
    "    MLPClassifier(hidden_layer_sizes=[100, 100]), # works\n",
    "    LinearDiscriminantAnalysis(), # works\n",
    "    LogisticRegression(), # works\n",
    "    QuadraticDiscriminantAnalysis(), # works\n",
    "]\n",
    "\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"F1 Score\", \"ROC\", \"Precision\", \"Recall\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X)\n",
    "    name = clf.__class__.__name__\n",
    "\n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    print('****Results****')\n",
    "\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc_ = accuracy_score(y_test, train_predictions)\n",
    "    acc = acc_.round(3)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "\n",
    "#     coef_scores = X_scaled\n",
    "#     coef_scores = clf.coef_\n",
    "#     print(coef_scores)\n",
    "\n",
    "    fbeta_ = fbeta_score(y_test, train_predictions, beta=1)\n",
    "    fbeta = fbeta_.round(3)\n",
    "    print(\"F1 Score: {}\".format(fbeta))\n",
    "\n",
    "    roc_ = roc_auc_score(y_test, train_predictions)\n",
    "    roc = roc_.round(3)\n",
    "    print(\"AUC (ROC) Score: {}\".format(roc))\n",
    "\n",
    "    precision_ = precision_score(y_test, train_predictions, average='binary')\n",
    "    precision = precision_.round(3)\n",
    "    print(\"Precision Score: {}\".format(precision))\n",
    "\n",
    "    recall_ = recall_score(y_test, train_predictions)\n",
    "    recall = recall_.round(3)\n",
    "    print(\"Recall Score: {}\".format(recall))\n",
    "\n",
    "#     train_predictions = clf.predict_proba(X_test)\n",
    "    ll_ = log_loss(y_test, train_predictions)\n",
    "    ll = ll_.round(3)\n",
    "#     print(\"Log Loss: {}\".format(ll))\n",
    "\n",
    "    log_entry = pd.DataFrame([[name, acc*100, fbeta, roc, precision, recall, ll]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "\n",
    "# print(\"=\"*30)\n",
    "# type(coef_scores)\n",
    "# print(index)\n",
    "# print(log_entry)\n",
    "# type(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803389a2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-02T02:48:28.801931Z",
     "iopub.status.idle": "2022-08-02T02:48:28.802931Z",
     "shell.execute_reply": "2022-08-02T02:48:28.802931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save DF as PNG\n",
    "def render_mpl_table(imputed_df, col_width=6.0, row_height=0.625, font_size=10,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(imputed_df.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=imputed_df.values, bbox=bbox, colLabels=imputed_df.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax\n",
    "\n",
    "fig,ax = render_mpl_table(log, header_columns=0, col_width=3.0)\n",
    "fig.savefig(\"table_mpl.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa06f3",
   "metadata": {},
   "source": [
    "### Setting up Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e2ba0faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:28.908186Z",
     "iopub.status.busy": "2022-08-02T02:48:28.907152Z",
     "iopub.status.idle": "2022-08-02T02:48:29.012100Z",
     "shell.execute_reply": "2022-08-02T02:48:29.011267Z",
     "shell.execute_reply.started": "2022-08-02T02:48:28.908186Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "893e416a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:48:29.016098Z",
     "iopub.status.busy": "2022-08-02T02:48:29.016098Z",
     "iopub.status.idle": "2022-08-02T02:48:29.131093Z",
     "shell.execute_reply": "2022-08-02T02:48:29.130244Z",
     "shell.execute_reply.started": "2022-08-02T02:48:29.016098Z"
    }
   },
   "outputs": [],
   "source": [
    "#set up the X matrix and y target variable\n",
    "X = bank_main_df.drop(columns='deposit')\n",
    "y = bank_main_df['deposit']\n",
    "\n",
    "#split the data appropriately into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate scaler and LogisticRegression model\n",
    "sc = StandardScaler()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#fit/transform the X_train and X_test datasets\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)\n",
    "\n",
    "#train the logreg model\n",
    "logreg.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04803a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score the model\n",
    "print(f\"Train score: {logreg.score(X_train_sc,y_train)}\")\n",
    "print(f\"Test score: {logreg.score(X_test_sc,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e5a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the test dataset, make predictions for the target variable\n",
    "y_preds = logreg.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Confusion matrix so we can find Type I / Type II errors:\\n{confusion_matrix(y_true=y_test, y_pred=y_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4948764",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here is a classification report, based on the confusion matrix\")\n",
    "print(classification_report(y_true=y_test,y_pred=y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563931c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,3))\n",
    "sns.barplot(X.columns,logreg.coef_[0])\n",
    "plt.xticks(rotation=60)\n",
    "plt.title(\"Extracting the Feature Importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b22af",
   "metadata": {},
   "source": [
    "### Further discussion for the group\n",
    "* **What further refinements to the dataset should we make as part of the EDA / cleanup?**\n",
    "    * Removing the *pdays* variable, for example\n",
    "    * Dropping outliers \n",
    "* **How might the use of other classification algorithms and scalers affect the final predictions?**\n",
    "    * ~Algorithms like LogisticRegression, DecisionTree, RandomForest, Kneighbors, NaiveBayes, neural net, etc.~\n",
    "    * Scalers like StandardScaler, MinMaxScaler, RobustScaler\n",
    "    * PCA (principal component analysis) to reduce dimensions\n",
    "* **Playing with parameters, pipelines, gridsearches to maximize True Negatives and minimize False Negatives**\n",
    "    * That is, maximize deposit==1 correct predictions and reducing deposit==0 wrong predictions\n",
    "    * Even if that means accidentally overpredicting the number of true deposits, better to try a bad path than miss a potential business opportunity\n",
    "* **Extending this to other predictions**\n",
    "    * e.g. predicting the \"default\" variable, or some other classification\n",
    "    * e.g. predicting a range for continuous values based on categorical values\n",
    "* **Best ways to impute missing data?**\n",
    "    * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f6a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
