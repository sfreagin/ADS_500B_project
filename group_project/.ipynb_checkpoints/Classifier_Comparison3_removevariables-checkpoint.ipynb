{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695e8e34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initial Prediction Model\n",
    "The original README file says:\n",
    "> Often, more than one contact to the same client was required, **in order to access if the product (bank term deposit) would be (or not) subscribed**\n",
    "\n",
    "Therefore, let's start with a simple binary classification model to predict Deposit yes/no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bf69acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import the right libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, accuracy_score, log_loss, roc_auc_score, hamming_loss, fbeta_score, auc, roc_curve, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, learning_curve, GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, LabelEncoder, RobustScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0837eb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this option just allwos us to see every column in the notebook\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#pd.get_option(\"display.max_columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "098561a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>deposit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58.0</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.0</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.0</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.0</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45206</th>\n",
       "      <td>51.0</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>825</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>17</td>\n",
       "      <td>nov</td>\n",
       "      <td>977</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45207</th>\n",
       "      <td>71.0</td>\n",
       "      <td>retired</td>\n",
       "      <td>divorced</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1729</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>17</td>\n",
       "      <td>nov</td>\n",
       "      <td>456</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45208</th>\n",
       "      <td>72.0</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>5715</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>17</td>\n",
       "      <td>nov</td>\n",
       "      <td>1127</td>\n",
       "      <td>5</td>\n",
       "      <td>184</td>\n",
       "      <td>3</td>\n",
       "      <td>success</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45209</th>\n",
       "      <td>57.0</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>668</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>17</td>\n",
       "      <td>nov</td>\n",
       "      <td>508</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45210</th>\n",
       "      <td>37.0</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2971</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>17</td>\n",
       "      <td>nov</td>\n",
       "      <td>361</td>\n",
       "      <td>2</td>\n",
       "      <td>188</td>\n",
       "      <td>11</td>\n",
       "      <td>other</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45211 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age           job   marital  education default  balance housing loan  \\\n",
       "0      58.0    management   married   tertiary      no     2143     yes   no   \n",
       "1      44.0    technician    single  secondary      no       29     yes   no   \n",
       "2      33.0  entrepreneur   married  secondary      no        2     yes  yes   \n",
       "3      47.0   blue-collar   married    unknown      no     1506     yes   no   \n",
       "4      33.0       unknown    single    unknown      no        1      no   no   \n",
       "...     ...           ...       ...        ...     ...      ...     ...  ...   \n",
       "45206  51.0    technician   married   tertiary      no      825      no   no   \n",
       "45207  71.0       retired  divorced    primary      no     1729      no   no   \n",
       "45208  72.0       retired   married  secondary      no     5715      no   no   \n",
       "45209  57.0   blue-collar   married  secondary      no      668      no   no   \n",
       "45210  37.0  entrepreneur   married  secondary      no     2971      no   no   \n",
       "\n",
       "         contact  day month  duration  campaign  pdays  previous poutcome  \\\n",
       "0        unknown    5   may       261         1     -1         0  unknown   \n",
       "1        unknown    5   may       151         1     -1         0  unknown   \n",
       "2        unknown    5   may        76         1     -1         0  unknown   \n",
       "3        unknown    5   may        92         1     -1         0  unknown   \n",
       "4            NaN    5   may       198         1     -1         0  unknown   \n",
       "...          ...  ...   ...       ...       ...    ...       ...      ...   \n",
       "45206   cellular   17   nov       977         3     -1         0  unknown   \n",
       "45207   cellular   17   nov       456         2     -1         0  unknown   \n",
       "45208   cellular   17   nov      1127         5    184         3  success   \n",
       "45209  telephone   17   nov       508         4     -1         0  unknown   \n",
       "45210   cellular   17   nov       361         2    188        11    other   \n",
       "\n",
       "      deposit  \n",
       "0          no  \n",
       "1          no  \n",
       "2          no  \n",
       "3          no  \n",
       "4          no  \n",
       "...       ...  \n",
       "45206     yes  \n",
       "45207     yes  \n",
       "45208     yes  \n",
       "45209      no  \n",
       "45210      no  \n",
       "\n",
       "[45211 rows x 17 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pull in the dataset and turn into a DataFrame\n",
    "bank_main_df = pd.read_csv('./Dataset_1_Bank Marketing/bank_marketing.csv',delimiter=';')\n",
    "bank_main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "468c0585",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>43872.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.924781</td>\n",
       "      <td>1362.272058</td>\n",
       "      <td>15.806419</td>\n",
       "      <td>258.163080</td>\n",
       "      <td>2.763841</td>\n",
       "      <td>40.197828</td>\n",
       "      <td>0.580323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.610835</td>\n",
       "      <td>3044.765829</td>\n",
       "      <td>8.322476</td>\n",
       "      <td>257.527812</td>\n",
       "      <td>3.098021</td>\n",
       "      <td>100.128746</td>\n",
       "      <td>2.303441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>-8019.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>95.000000</td>\n",
       "      <td>102127.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>4918.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>275.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        balance           day      duration      campaign  \\\n",
       "count  43872.000000   45211.000000  45211.000000  45211.000000  45211.000000   \n",
       "mean      40.924781    1362.272058     15.806419    258.163080      2.763841   \n",
       "std       10.610835    3044.765829      8.322476    257.527812      3.098021   \n",
       "min       18.000000   -8019.000000      1.000000      0.000000      1.000000   \n",
       "25%       33.000000      72.000000      8.000000    103.000000      1.000000   \n",
       "50%       39.000000     448.000000     16.000000    180.000000      2.000000   \n",
       "75%       48.000000    1428.000000     21.000000    319.000000      3.000000   \n",
       "max       95.000000  102127.000000     31.000000   4918.000000     63.000000   \n",
       "\n",
       "              pdays      previous  \n",
       "count  45211.000000  45211.000000  \n",
       "mean      40.197828      0.580323  \n",
       "std      100.128746      2.303441  \n",
       "min       -1.000000      0.000000  \n",
       "25%       -1.000000      0.000000  \n",
       "50%       -1.000000      0.000000  \n",
       "75%       -1.000000      0.000000  \n",
       "max      871.000000    275.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_main_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "957ec883-cb54-4d28-a94e-aa381126d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a field that determines whether or not the customer was ever previously contacted\n",
    "bank_main_df['prior_contact'] = [ 0 if bank_main_df['pdays'][i] == -1 else 1 for i in range(len(bank_main_df))]\n",
    "\n",
    "#replacing the yes/no categorical values with 1/0 binary digits\n",
    "bank_main_df['deposit'] = [1 if (bank_main_df['deposit'][i] == 'yes') else 0 for i in range(len(bank_main_df)) ]\n",
    "\n",
    "#convert the \"day\" field to a categorical variable\n",
    "bank_main_df['day'] = pd.Categorical(bank_main_df['day'])\n",
    "\n",
    "#dropping pdays and previous, because the important information is captured in prior_contact\n",
    "bank_main_df.drop(columns=['pdays','previous'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "200492f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#because we have so many cateogrical variables, we should one-hot encode them (i.e. create dummy categorical variables)\n",
    "#we also use drop_first=True to reduce the redundant column count \n",
    "bank_main_df = pd.get_dummies(bank_main_df, drop_first=False)\n",
    "\n",
    "# bank_main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef8251dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#note that only the \"age\" category has null values\n",
    "\n",
    "# pd.isnull(bank_main_df).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50f2a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T02:22:26.864109Z",
     "iopub.status.busy": "2022-08-02T02:22:26.864109Z",
     "iopub.status.idle": "2022-08-02T02:22:26.995149Z",
     "shell.execute_reply": "2022-08-02T02:22:26.995149Z",
     "shell.execute_reply.started": "2022-08-02T02:22:26.864109Z"
    }
   },
   "source": [
    "# Imputing the missing values in \"Age\" variable \n",
    "\n",
    "* **Iterative Imputer:**\n",
    "Multivariate imputer that estimates each feature from all the others. A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea6ce845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use multivariate imputer that estimates and imputes null values based on all the others. \n",
    "\n",
    "imp = IterativeImputer(max_iter=10, verbose=0) # values passed are defaults, but added them because they seem important... play around\n",
    "imp.fit(bank_main_df)\n",
    "imputed_df = imp.transform(bank_main_df)\n",
    "imputed_df = pd.DataFrame(imputed_df, columns=bank_main_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb889199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.isnull(imputed_df).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb1b28",
   "metadata": {},
   "source": [
    "# R-Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb10afd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#scaler = StandardScaler() \n",
    "#X_scaled = scaler.fit_transform(imputed_df)\n",
    "\n",
    "#X = imputed_df.drop(columns='deposit')\n",
    "#y = imputed_df['deposit']\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, shuffle=True, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d906b58",
   "metadata": {},
   "source": [
    "# Loop through all classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e533f-1cb9-43a6-9f05-1cc4ec91c3fd",
   "metadata": {},
   "source": [
    "### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94aa52ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# StandardScaler, MinMaxScaler, RobustScaler\n",
    "#scaler = RobustScaler() \n",
    "#X_scaled = scaler.fit_transform(imputed_df)\n",
    "\n",
    "X = imputed_df.drop(columns='deposit')\n",
    "y = imputed_df['deposit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, shuffle=True)\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21430af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KNeighborsClassifier\n",
      "****Results****\n",
      "Accuracy: 88.7000%\n",
      "F1 Score: 0.352\n",
      "AUC (ROC) Score: 0.617\n",
      "Precision Score: 0.517\n",
      "Recall Score: 0.266\n",
      "\n",
      "Classic train score: 0.9262\n",
      "Classic test score: 0.8871\n",
      "Confusion matrix: \n",
      "[[11618   387]\n",
      " [ 1144   415]]\n",
      "\n",
      "============================================================\n",
      "DecisionTreeClassifier\n",
      "****Results****\n",
      "Accuracy: 87.1000%\n",
      "F1 Score: 0.466\n",
      "AUC (ROC) Score: 0.704\n",
      "Precision Score: 0.446\n",
      "Recall Score: 0.487\n",
      "\n",
      "Classic train score: 1.0\n",
      "Classic test score: 0.8714\n",
      "Confusion matrix: \n",
      "[[11060   945]\n",
      " [  799   760]]\n",
      "\n",
      "============================================================\n",
      "RandomForestClassifier\n",
      "****Results****\n",
      "Accuracy: 90.2000%\n",
      "F1 Score: 0.449\n",
      "AUC (ROC) Score: 0.661\n",
      "Precision Score: 0.64\n",
      "Recall Score: 0.346\n",
      "\n",
      "Classic train score: 1.0\n",
      "Classic test score: 0.9025\n",
      "Confusion matrix: \n",
      "[[11701   304]\n",
      " [ 1019   540]]\n",
      "\n",
      "============================================================\n",
      "AdaBoostClassifier\n",
      "****Results****\n",
      "Accuracy: 89.8000%\n",
      "F1 Score: 0.452\n",
      "AUC (ROC) Score: 0.667\n",
      "Precision Score: 0.59\n",
      "Recall Score: 0.367\n",
      "\n",
      "Classic train score: 0.9014\n",
      "Classic test score: 0.8979\n",
      "Confusion matrix: \n",
      "[[11607   398]\n",
      " [  987   572]]\n",
      "\n",
      "============================================================\n",
      "GradientBoostingClassifier\n",
      "****Results****\n",
      "Accuracy: 90.4000%\n",
      "F1 Score: 0.484\n",
      "AUC (ROC) Score: 0.682\n",
      "Precision Score: 0.631\n",
      "Recall Score: 0.393\n",
      "\n",
      "Classic train score: 0.9105\n",
      "Classic test score: 0.9038\n",
      "Confusion matrix: \n",
      "[[11646   359]\n",
      " [  946   613]]\n",
      "\n",
      "============================================================\n",
      "GaussianNB\n",
      "****Results****\n",
      "Accuracy: 82.8000%\n",
      "F1 Score: 0.419\n",
      "AUC (ROC) Score: 0.703\n",
      "Precision Score: 0.342\n",
      "Recall Score: 0.54\n",
      "\n",
      "Classic train score: 0.8306\n",
      "Classic test score: 0.8279\n",
      "Confusion matrix: \n",
      "[[10388  1617]\n",
      " [  717   842]]\n",
      "\n",
      "============================================================\n",
      "BernoulliNB\n",
      "****Results****\n",
      "Accuracy: 85.7000%\n",
      "F1 Score: 0.403\n",
      "AUC (ROC) Score: 0.667\n",
      "Precision Score: 0.388\n",
      "Recall Score: 0.42\n",
      "\n",
      "Classic train score: 0.8564\n",
      "Classic test score: 0.8572\n",
      "Confusion matrix: \n",
      "[[10972  1033]\n",
      " [  904   655]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reagins\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MLPClassifier\n",
      "****Results****\n",
      "Accuracy: 89.8000%\n",
      "F1 Score: 0.519\n",
      "AUC (ROC) Score: 0.716\n",
      "Precision Score: 0.566\n",
      "Recall Score: 0.48\n",
      "\n",
      "Classic train score: 0.9672\n",
      "Classic test score: 0.898\n",
      "Confusion matrix: \n",
      "[[11432   573]\n",
      " [  811   748]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifiers = [\n",
    "#     MultinomialNB(), # doesn't work\n",
    "    KNeighborsClassifier(3), # works\n",
    "#     SVC(kernel=\"rbf\", C=0.001, probability=True), # took a long time... need to refresh memory\n",
    "#     SVC(kernel='linear'), # took a long time... need to refresh memory\n",
    "#     NuSVC(probability=True, nu=0.1), # took a long time... need to refresh memory\n",
    "    DecisionTreeClassifier(), # works\n",
    "    RandomForestClassifier(), # works\n",
    "    AdaBoostClassifier(), # works\n",
    "    GradientBoostingClassifier(), # works\n",
    "    GaussianNB(), # works\n",
    "    BernoulliNB(), # works\n",
    "    MLPClassifier(), # works\n",
    "    MLPClassifier(hidden_layer_sizes=[100, 100]), # works\n",
    "    LinearDiscriminantAnalysis(), # works\n",
    "    LogisticRegression(), # works\n",
    "    QuadraticDiscriminantAnalysis(), # works\n",
    "]\n",
    "\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"F1 Score\", \"ROC\", \"Precision\", \"Recall\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X)\n",
    "    name = clf.__class__.__name__\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(name)\n",
    "    print('****Results****')\n",
    "\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc_ = accuracy_score(y_test, train_predictions)\n",
    "    acc = acc_.round(3)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "\n",
    "#     coef_scores = X_scaled\n",
    "#     coef_scores = clf.coef_\n",
    "#     print(coef_scores)\n",
    "\n",
    "    fbeta_ = fbeta_score(y_test, train_predictions, beta=1)\n",
    "    fbeta = fbeta_.round(3)\n",
    "    print(\"F1 Score: {}\".format(fbeta))\n",
    "\n",
    "    roc_ = roc_auc_score(y_test, train_predictions)\n",
    "    roc = roc_.round(3)\n",
    "    print(\"AUC (ROC) Score: {}\".format(roc))\n",
    "\n",
    "    precision_ = precision_score(y_test, train_predictions, average='binary')\n",
    "    precision = precision_.round(3)\n",
    "    print(\"Precision Score: {}\".format(precision))\n",
    "\n",
    "    recall_ = recall_score(y_test, train_predictions)\n",
    "    recall = recall_.round(3)\n",
    "    print(\"Recall Score: {}\".format(recall))\n",
    "    \n",
    "    print(f\"\\nClassic train score: {np.round(clf.score(X_train, y_train),4)}\")\n",
    "    print(f\"Classic test score: {np.round(clf.score(X_test, y_test),4)}\")\n",
    "    confusion_matrix_ = confusion_matrix(y_test, train_predictions)\n",
    "    print(f\"Confusion matrix: \\n{confusion_matrix_}\\n\")\n",
    "\n",
    "#     train_predictions = clf.predict_proba(X_test)\n",
    "    ll_ = log_loss(y_test, train_predictions)\n",
    "    ll = ll_.round(3)\n",
    "#     print(\"Log Loss: {}\".format(ll))\n",
    "\n",
    "    log_entry = pd.DataFrame([[name, acc*100, fbeta, roc, precision, recall, ll]], columns=log_cols)\n",
    "    log = pd.concat([log,log_entry])\n",
    "\n",
    "# print(\"=\"*30)\n",
    "# type(coef_scores)\n",
    "# print(index)\n",
    "# print(log_entry)\n",
    "# type(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803389a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save DF as PNG\n",
    "def render_mpl_table(imputed_df, col_width=6.0, row_height=0.625, font_size=10,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(imputed_df.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=imputed_df.values, bbox=bbox, colLabels=imputed_df.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax\n",
    "\n",
    "fig,ax = render_mpl_table(log, header_columns=0, col_width=3.0)\n",
    "fig.savefig(\"table_mpl.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32bf79-c65d-4e99-ab9c-d1bdd3b94e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "log1 = log.set_index('Classifier')\n",
    "\n",
    "norm1_df = log1 / log1.max(0)\n",
    "\n",
    "sns.heatmap(norm_df.astype('float'),cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c02556-60d2-49fe-954b-e3ae4c8752c0",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0941b7-57cf-425f-a83a-87b7446d0059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# StandardScaler, MinMaxScaler, RobustScaler\n",
    "#scaler = RobustScaler() \n",
    "#X_scaled = scaler.fit_transform(imputed_df)\n",
    "\n",
    "X = imputed_df.drop(columns='deposit')\n",
    "y = imputed_df['deposit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246b284-66a2-4960-b529-f7f14f2fd130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "classifiers = [\n",
    "#     MultinomialNB(), # doesn't work\n",
    "    KNeighborsClassifier(3), # works\n",
    "#     SVC(kernel=\"rbf\", C=0.001, probability=True), # took a long time... need to refresh memory\n",
    "#     SVC(kernel='linear'), # took a long time... need to refresh memory\n",
    "#     NuSVC(probability=True, nu=0.1), # took a long time... need to refresh memory\n",
    "    DecisionTreeClassifier(), # works\n",
    "    RandomForestClassifier(), # works\n",
    "    AdaBoostClassifier(), # works\n",
    "    GradientBoostingClassifier(), # works\n",
    "    GaussianNB(), # works\n",
    "    BernoulliNB(), # works\n",
    "    MLPClassifier(), # works\n",
    "    MLPClassifier(hidden_layer_sizes=[100, 100]), # works\n",
    "    LinearDiscriminantAnalysis(), # works\n",
    "    LogisticRegression(), # works\n",
    "    QuadraticDiscriminantAnalysis(), # works\n",
    "]\n",
    "\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"F1 Score\", \"ROC\", \"Precision\", \"Recall\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X)\n",
    "    name = clf.__class__.__name__\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(name)\n",
    "    print('****Results****')\n",
    "\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc_ = accuracy_score(y_test, train_predictions)\n",
    "    acc = acc_.round(3)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "\n",
    "#     coef_scores = X_scaled\n",
    "#     coef_scores = clf.coef_\n",
    "#     print(coef_scores)\n",
    "\n",
    "    fbeta_ = fbeta_score(y_test, train_predictions, beta=1)\n",
    "    fbeta = fbeta_.round(3)\n",
    "    print(\"F1 Score: {}\".format(fbeta))\n",
    "\n",
    "    roc_ = roc_auc_score(y_test, train_predictions)\n",
    "    roc = roc_.round(3)\n",
    "    print(\"AUC (ROC) Score: {}\".format(roc))\n",
    "\n",
    "    precision_ = precision_score(y_test, train_predictions, average='binary')\n",
    "    precision = precision_.round(3)\n",
    "    print(\"Precision Score: {}\".format(precision))\n",
    "\n",
    "    recall_ = recall_score(y_test, train_predictions)\n",
    "    recall = recall_.round(3)\n",
    "    print(\"Recall Score: {}\".format(recall))\n",
    "    \n",
    "    print(f\"\\nClassic train score: {np.round(clf.score(X_train, y_train),4)}\")\n",
    "    print(f\"Classic test score: {np.round(clf.score(X_test, y_test),4)}\")\n",
    "    confusion_matrix_ = confusion_matrix(y_test, train_predictions)\n",
    "    print(f\"Confusion matrix: \\n{confusion_matrix_}\\n\")\n",
    "\n",
    "#     train_predictions = clf.predict_proba(X_test)\n",
    "    ll_ = log_loss(y_test, train_predictions)\n",
    "    ll = ll_.round(3)\n",
    "#     print(\"Log Loss: {}\".format(ll))\n",
    "\n",
    "    log_entry = pd.DataFrame([[name, acc*100, fbeta, roc, precision, recall, ll]], columns=log_cols)\n",
    "    log = pd.concat([log,log_entry])\n",
    "\n",
    "# print(\"=\"*30)\n",
    "# type(coef_scores)\n",
    "# print(index)\n",
    "# print(log_entry)\n",
    "# type(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c3100-45b4-435f-8f2f-42386413e0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save DF as PNG\n",
    "def render_mpl_table(imputed_df, col_width=6.0, row_height=0.625, font_size=10,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(imputed_df.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=imputed_df.values, bbox=bbox, colLabels=imputed_df.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax\n",
    "\n",
    "fig,ax = render_mpl_table(log, header_columns=0, col_width=3.0)\n",
    "fig.savefig(\"table_mpl.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb589f-1a08-4696-8170-f0e000bd3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "log2 = log.set_index('Classifier')\n",
    "\n",
    "norm2_df = log2 / log2.max(0)\n",
    "\n",
    "sns.heatmap(norm_df.astype('float'),cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97e077-9aba-4a97-a76f-46f0bf174947",
   "metadata": {},
   "source": [
    "### RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d6efe-aa1e-4b2d-a13b-3e5200cd3275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# StandardScaler, MinMaxScaler, RobustScaler\n",
    "#scaler = RobustScaler() \n",
    "#X_scaled = scaler.fit_transform(imputed_df)\n",
    "\n",
    "X = imputed_df.drop(columns='deposit')\n",
    "y = imputed_df['deposit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, shuffle=True)\n",
    "\n",
    "scaler = RobustScaler() \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ca2e8-2f4f-475e-a445-7d29945077e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "classifiers = [\n",
    "#     MultinomialNB(), # doesn't work\n",
    "    KNeighborsClassifier(3), # works\n",
    "#     SVC(kernel=\"rbf\", C=0.001, probability=True), # took a long time... need to refresh memory\n",
    "#     SVC(kernel='linear'), # took a long time... need to refresh memory\n",
    "#     NuSVC(probability=True, nu=0.1), # took a long time... need to refresh memory\n",
    "    DecisionTreeClassifier(), # works\n",
    "    RandomForestClassifier(), # works\n",
    "    AdaBoostClassifier(), # works\n",
    "    GradientBoostingClassifier(), # works\n",
    "    GaussianNB(), # works\n",
    "    BernoulliNB(), # works\n",
    "    MLPClassifier(), # works\n",
    "    MLPClassifier(hidden_layer_sizes=[100, 100]), # works\n",
    "    LinearDiscriminantAnalysis(), # works\n",
    "    LogisticRegression(), # works\n",
    "    QuadraticDiscriminantAnalysis(), # works\n",
    "]\n",
    "\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"F1 Score\", \"ROC\", \"Precision\", \"Recall\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X)\n",
    "    name = clf.__class__.__name__\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(name)\n",
    "    print('****Results****')\n",
    "\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc_ = accuracy_score(y_test, train_predictions)\n",
    "    acc = acc_.round(3)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "\n",
    "#     coef_scores = X_scaled\n",
    "#     coef_scores = clf.coef_\n",
    "#     print(coef_scores)\n",
    "\n",
    "    fbeta_ = fbeta_score(y_test, train_predictions, beta=1)\n",
    "    fbeta = fbeta_.round(3)\n",
    "    print(\"F1 Score: {}\".format(fbeta))\n",
    "\n",
    "    roc_ = roc_auc_score(y_test, train_predictions)\n",
    "    roc = roc_.round(3)\n",
    "    print(\"AUC (ROC) Score: {}\".format(roc))\n",
    "\n",
    "    precision_ = precision_score(y_test, train_predictions, average='binary')\n",
    "    precision = precision_.round(3)\n",
    "    print(\"Precision Score: {}\".format(precision))\n",
    "\n",
    "    recall_ = recall_score(y_test, train_predictions)\n",
    "    recall = recall_.round(3)\n",
    "    print(\"Recall Score: {}\".format(recall))\n",
    "    \n",
    "    print(f\"\\nClassic train score: {np.round(clf.score(X_train, y_train),4)}\")\n",
    "    print(f\"Classic test score: {np.round(clf.score(X_test, y_test),4)}\")\n",
    "    confusion_matrix_ = confusion_matrix(y_test, train_predictions)\n",
    "    print(f\"Confusion matrix: \\n{confusion_matrix_}\\n\")\n",
    "\n",
    "#     train_predictions = clf.predict_proba(X_test)\n",
    "    ll_ = log_loss(y_test, train_predictions)\n",
    "    ll = ll_.round(3)\n",
    "#     print(\"Log Loss: {}\".format(ll))\n",
    "\n",
    "    log_entry = pd.DataFrame([[name, acc*100, fbeta, roc, precision, recall, ll]], columns=log_cols)\n",
    "    log = pd.concat([log,log_entry])\n",
    "\n",
    "# print(\"=\"*30)\n",
    "# type(coef_scores)\n",
    "# print(index)\n",
    "# print(log_entry)\n",
    "# type(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a59e89-86b3-4c55-be65-3d36e64d8e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save DF as PNG\n",
    "def render_mpl_table(imputed_df, col_width=6.0, row_height=0.625, font_size=10,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(imputed_df.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=imputed_df.values, bbox=bbox, colLabels=imputed_df.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax\n",
    "\n",
    "fig,ax = render_mpl_table(log, header_columns=0, col_width=3.0)\n",
    "fig.savefig(\"table_mpl.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37265873-2565-4345-8a4f-d2fd114fb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log3 = log.set_index('Classifier')\n",
    "\n",
    "norm3_df = log3 / log3.max(0)\n",
    "\n",
    "sns.heatmap(norm_df.astype('float'),cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563931c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(24,3))\n",
    "#sns.barplot(X.columns,logreg.coef_[0])\n",
    "#plt.xticks(rotation=60)\n",
    "#plt.title(\"Extracting the Feature Importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b22af",
   "metadata": {},
   "source": [
    "### Further discussion for the group\n",
    "* **What further refinements to the dataset should we make as part of the EDA / cleanup?**\n",
    "    * Removing the *pdays* variable, for example\n",
    "    * Dropping outliers \n",
    "* **How might the use of other classification algorithms and scalers affect the final predictions?**\n",
    "    * ~Algorithms like LogisticRegression, DecisionTree, RandomForest, Kneighbors, NaiveBayes, neural net, etc.~\n",
    "    * Scalers like StandardScaler, MinMaxScaler, RobustScaler\n",
    "    * PCA (principal component analysis) to reduce dimensions\n",
    "* **Playing with parameters, pipelines, gridsearches to maximize True Negatives and minimize False Negatives**\n",
    "    * That is, maximize deposit==1 correct predictions and reducing deposit==0 wrong predictions\n",
    "    * Even if that means accidentally overpredicting the number of true deposits, better to try a bad path than miss a potential business opportunity\n",
    "* **Extending this to other predictions**\n",
    "    * e.g. predicting the \"default\" variable, or some other classification\n",
    "    * e.g. predicting a range for continuous values based on categorical values\n",
    "* **Best ways to impute missing data?**\n",
    "    * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f6a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc140aa-dcd4-45e8-8360-bb158b089330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b029a36-60e8-439d-95ff-d93ab93ec608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
